parameters:
    elasticsearch_indexes:
        - fr
        - en
    elasticsearch_fr_index:
        number_of_shards: 5
        number_of_replicas: 1
        analysis:
            analyzer:
                default_analyzer :
                    type     :    custom
                    tokenizer:    nGram
                    filter   :    [ asciifolding ,lowercase, elision, worddelimiter]
                default_search_analyzer :
                    type     :    custom
                    tokenizer:    standard
                    filter   :    [ asciifolding ,lowercase, elision, worddelimiter]
                fr_analyzer :
                    type     :    custom
                    tokenizer:    nGram
                    filter   :    [stopwords_fr, asciifolding ,lowercase, snowball_fr, elision, worddelimiter]
                fr_search_analyzer :
                    type     :    custom
                    tokenizer:    standard
                    filter   :    [stopwords_fr, asciifolding ,lowercase, snowball_fr, elision, worddelimiter]
            tokenizer:
                nGram:
                    type:     nGram
                    min_gram: 2
                    max_gram: 30
            filter:
                snowball:
                    type:     snowball
                    language: French
                elision:
                    type:     elision
                    articles: [l, m, t, qu, n, s, j, d]
                stopwords:
                    type:      stop
                    stopwords: [_french_]
                    ignore_case : true
                worddelimiter :
                    type:      word_delimiter
    elasticsearch_en_index:
        number_of_shards: 5
        number_of_replicas: 1
        analysis:
            analyzer:
                default_analyzer :
                    type     :    custom
                    tokenizer:    nGram
                    filter   :    [ asciifolding ,lowercase, elision, worddelimiter]
                default_search_analyzer :
                    type     :    custom
                    tokenizer:    standard
                    filter   :    [ asciifolding ,lowercase, elision, worddelimiter]
                en_analyzer :
                    type     :    custom
                    tokenizer:    nGram
                    filter   :    [stopwords_en, asciifolding ,lowercase, snowball_en, elision, worddelimiter]
                en_search_analyzer :
                    type     :    custom
                    tokenizer:    standard
                    filter   :    [stopwords_en, asciifolding ,lowercase, snowball_en, elision, worddelimiter]
                tokenizer:
                  nGram:
                    type:     nGram
                    min_gram: 2
                    max_gram: 30
            filter:
                snowball_en:
                    type:     snowball
                    language: English
                elision:
                    type:     elision
                    articles: [l, m, t, qu, n, s, j, d]
                stopwords_en:
                    type:      stop
                    stopwords: [_english_]
                    ignore_case : true
                worddelimiter :
                    type:      word_delimiter